#!/usr/bin/env python3
"""
Q&Aãƒšã‚¢ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ChromaDBã«ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import sys
from pathlib import Path
from typing import List, Dict
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class QAVectorizer:
    """Q&Aãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        persist_directory: str = None
    ):
        """
        åˆæœŸåŒ–

        Args:
            model_name: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
            persist_directory: ChromaDBã®æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print(f"ğŸ“¥ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name}")
        self.model = SentenceTransformer(model_name)

        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        if persist_directory is None:
            persist_directory = str(project_root / "data" / "chroma_db")

        self.persist_directory = persist_directory
        self.client = chromadb.Client(Settings(
            persist_directory=persist_directory,
            anonymized_telemetry=False
        ))

        print(f"ğŸ’¾ ChromaDB: {persist_directory}")

    def load_qa_pairs(self) -> List[Dict]:
        """
        catalog.jsonã‹ã‚‰Q&Aãƒšã‚¢ã‚’èª­ã¿è¾¼ã‚€

        Returns:
            Q&Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
        """
        catalog_path = project_root / "data" / "catalog.json"

        with open(catalog_path, 'r', encoding='utf-8') as f:
            catalog = json.load(f)

        all_qa_pairs = []

        for paper_id, paper_data in catalog['papers'].items():
            qa_pairs = paper_data.get('qa_pairs', [])

            if not qa_pairs:
                continue

            for idx, qa in enumerate(qa_pairs):
                # Q&Aãƒšã‚¢ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                qa_with_metadata = {
                    'id': f"{paper_id}_qa_{idx}",
                    'paper_id': paper_id,
                    'question': qa['question'],
                    'answer': qa['answer'],
                    'section': qa.get('section', 'unknown'),
                    'importance': qa.get('importance', 'medium'),
                    'keywords': qa.get('keywords', []),
                    # è«–æ–‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚è¿½åŠ 
                    'paper_title': paper_data.get('title', 'N/A'),
                    'paper_year': paper_data.get('year', 'N/A'),
                    'study_type': paper_data.get('study_type', 'N/A'),
                    'disease': paper_data.get('perspectives', {}).get('disease', 'N/A'),
                    'method': paper_data.get('perspectives', {}).get('method', 'N/A'),
                }
                all_qa_pairs.append(qa_with_metadata)

        print(f"ğŸ“Š èª­ã¿è¾¼ã‚“ã Q&Aãƒšã‚¢æ•°: {len(all_qa_pairs)}")
        return all_qa_pairs

    def vectorize_and_store(
        self,
        collection_name: str = "medical_papers_qa"
    ):
        """
        Q&Aãƒšã‚¢ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ChromaDBã«ä¿å­˜

        Args:
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        """
        # Q&Aãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿
        qa_pairs = self.load_qa_pairs()

        if not qa_pairs:
            print("âš ï¸  Q&Aãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« generate_qa.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return

        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
        try:
            # æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
            self.client.delete_collection(name=collection_name)
            print(f"ğŸ—‘ï¸  æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        except:
            pass

        collection = self.client.create_collection(
            name=collection_name,
            metadata={"description": "Medical papers Q&A pairs"}
        )

        print(f"ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’é–‹å§‹...")

        # ãƒãƒƒãƒå‡¦ç†
        batch_size = 100
        for i in range(0, len(qa_pairs), batch_size):
            batch = qa_pairs[i:i+batch_size]

            # è³ªå•ã¨å›ç­”ã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            texts = [
                f"Question: {qa['question']}\nAnswer: {qa['answer']}"
                for qa in batch
            ]

            # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
            embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            metadatas = [
                {
                    'paper_id': qa['paper_id'],
                    'question': qa['question'],
                    'answer': qa['answer'],
                    'section': qa['section'],
                    'importance': qa['importance'],
                    'keywords': json.dumps(qa['keywords'], ensure_ascii=False),
                    'paper_title': qa['paper_title'],
                    'paper_year': str(qa['paper_year']),
                    'study_type': qa['study_type'],
                    'disease': qa['disease'],
                    'method': qa['method'],
                }
                for qa in batch
            ]

            # IDãƒªã‚¹ãƒˆ
            ids = [qa['id'] for qa in batch]

            # ChromaDBã«ä¿å­˜
            collection.add(
                embeddings=embeddings,
                metadatas=metadatas,
                documents=texts,
                ids=ids
            )

            print(f"  âœ… {i+len(batch)}/{len(qa_pairs)} ä»¶å‡¦ç†å®Œäº†")

        print(f"\nğŸ‰ ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†ï¼")
        print(f"   ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {collection_name}")
        print(f"   ç·Q&Aæ•°: {len(qa_pairs)}")
        print(f"   ä¿å­˜å…ˆ: {self.persist_directory}")

    def search(
        self,
        query: str,
        n_results: int = 5,
        collection_name: str = "medical_papers_qa",
        filter_metadata: Dict = None
    ) -> List[Dict]:
        """
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            n_results: è¿”ã™çµæœæ•°
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            filter_metadata: ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ï¼ˆä¾‹: {"disease": "stroke"}ï¼‰

        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        collection = self.client.get_collection(name=collection_name)

        # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_embedding = self.model.encode([query], convert_to_numpy=True).tolist()

        # æ¤œç´¢å®Ÿè¡Œ
        results = collection.query(
            query_embeddings=query_embedding,
            n_results=n_results,
            where=filter_metadata
        )

        # çµæœã‚’æ•´å½¢
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'id': results['ids'][0][i],
                'distance': results['distances'][0][i],
                'metadata': results['metadatas'][0][i],
                'document': results['documents'][0][i]
            })

        return formatted_results


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Q&Aãƒšã‚¢ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–')
    parser.add_argument(
        '--model',
        default='paraphrase-multilingual-MiniLM-L12-v2',
        help='åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å'
    )
    parser.add_argument(
        '--collection',
        default='medical_papers_qa',
        help='ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å'
    )

    args = parser.parse_args()

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Ÿè¡Œ
    vectorizer = QAVectorizer(model_name=args.model)
    vectorizer.vectorize_and_store(collection_name=args.collection)

    print("\nâœ… å®Œäº†ã—ã¾ã—ãŸï¼")
    print("   æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Streamlit UIã§ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨ã§ãã¾ã™")


if __name__ == '__main__':
    main()
